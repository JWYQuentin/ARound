{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4R43mLqcvRO",
        "outputId": "60f65eff-cb0f-462a-9742-05ae489ca916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EtqHhbtc5J-",
        "outputId": "1fedda6a-ed86-4de1-b779-cee58193cf03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b28503376d6d>:7: DtypeWarning: Columns (2,3,6,7,8,9,10,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(dataset_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the dataset:\n",
            "   Event Date                              Headliner  \\\n",
            "0  2024-09-18                                  Creed   \n",
            "1  2024-09-14                                  Creed   \n",
            "2  2024-09-13  Bruce Springsteen & The E Street Band   \n",
            "3  2024-09-13                                  Creed   \n",
            "4  2024-09-13                Billy Joel, Rod Stewart   \n",
            "\n",
            "                          sp artist_name  \\\n",
            "0                                  Creed   \n",
            "1                                  Creed   \n",
            "2  Bruce Springsteen & The E Street Band   \n",
            "3                                  Creed   \n",
            "4                Billy Joel, Rod Stewart   \n",
            "\n",
            "                                     sp artist_genre  sp followers  \\\n",
            "0  ['alternative metal', 'nu metal', 'post-grunge...     3527070.0   \n",
            "1  ['alternative metal', 'nu metal', 'post-grunge...     3527070.0   \n",
            "2  ['heartland rock', 'mellow gold', 'permanent w...     6567386.0   \n",
            "3  ['alternative metal', 'nu metal', 'post-grunge...     3527070.0   \n",
            "4  ['album rock', 'classic rock', 'mellow gold', ...     6312751.0   \n",
            "\n",
            "   sp popularity                                yt name  \\\n",
            "0           74.0                                  Creed   \n",
            "1           74.0                                  Creed   \n",
            "2           79.0  Bruce Springsteen & The E Street Band   \n",
            "3           74.0                                  Creed   \n",
            "4           79.0                Billy Joel, Rod Stewart   \n",
            "\n",
            "              yt Channel ID           yt Title  \\\n",
            "0  UCP-tFf_VMQzhyeKMONL1KvQ              Creed   \n",
            "1  UCP-tFf_VMQzhyeKMONL1KvQ              Creed   \n",
            "2  UCcu7ANuD9J7hnTQCREqIc4Q  Bruce Springsteen   \n",
            "3  UCP-tFf_VMQzhyeKMONL1KvQ              Creed   \n",
            "4  UC0yYX8_4R-pITDGp0YC_Qfg      Rock Playlist   \n",
            "\n",
            "                                      yt Description  ...       Genre  \\\n",
            "0  Subscribe to Creed's Official Youtube Channel ...  ...  Pop / Rock   \n",
            "1  Subscribe to Creed's Official Youtube Channel ...  ...  Pop / Rock   \n",
            "2     Bruce Springsteen's official YouTube channel.   ...  Pop / Rock   \n",
            "3  Subscribe to Creed's Official Youtube Channel ...  ...  Pop / Rock   \n",
            "4                                                NaN  ...  Pop / Rock   \n",
            "\n",
            "   Avg. Tickets Sold  Avg. Gross USD  Avg. Event Capacity  Avg. Capacity Sold  \\\n",
            "0            20295.0       1228939.0              20295.0                100%   \n",
            "1            16308.0       1374174.0              16308.0                100%   \n",
            "2            39646.0       6556587.0              39646.0                100%   \n",
            "3            14995.0       1402969.0              14995.0                100%   \n",
            "4            44553.0       9676590.0              44553.0                100%   \n",
            "\n",
            "   Ticket Price Min USD  Ticket Price Max USD  Ticket Price Avg. USD  Month  \\\n",
            "0                  39.5                 225.0                  60.55      9   \n",
            "1                  39.5                 225.0                  84.26      9   \n",
            "2                  49.5                 299.5                 165.38      9   \n",
            "3                  39.5                 225.0                  93.56      9   \n",
            "4                  69.5                 349.5                 217.19      9   \n",
            "\n",
            "   day_of_week  \n",
            "0            2  \n",
            "1            5  \n",
            "2            4  \n",
            "3            4  \n",
            "4            4  \n",
            "\n",
            "[5 rows x 53 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/combined_df.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "train_df = df[\n",
        "        (df['Year'] >= 2020) &\n",
        "        (df['Headliner'].str.contains('\"', na=False)) &\n",
        "        (~df['Support'].isna()) &\n",
        "        (df['Genre'] != 'Family Entertainment')\n",
        "    ]\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N1wCWynbf3JA",
        "outputId": "1b76959f-3fde-4ac4-9a23-d961f0bf5d44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary statistics of the dataset:\n",
            "       sp followers  sp popularity  yt View Count  yt Subscriber Count  \\\n",
            "count  3.245600e+04   32456.000000   3.210200e+04         3.210200e+04   \n",
            "mean   2.498679e+06      51.025357   7.947109e+08         1.368386e+06   \n",
            "std    9.005023e+06      20.372168   3.018964e+09         5.633076e+06   \n",
            "min    0.000000e+00       0.000000   0.000000e+00         0.000000e+00   \n",
            "25%    6.217250e+04      40.000000   2.319628e+06         7.340000e+03   \n",
            "50%    3.271130e+05      53.000000   3.686581e+07         7.660000e+04   \n",
            "75%    1.490683e+06      65.000000   3.129296e+08         4.710000e+05   \n",
            "max    1.236853e+08     100.000000   6.104600e+10         3.210000e+08   \n",
            "\n",
            "       yt Video Count  Total population  Under 5 years population  \\\n",
            "count    32102.000000      3.249400e+04              32494.000000   \n",
            "mean       255.025263      8.631460e+05              50734.760448   \n",
            "std       1892.338983      1.569479e+06              95171.317352   \n",
            "min          0.000000      0.000000e+00                  0.000000   \n",
            "25%         32.000000      9.482600e+04               4942.000000   \n",
            "50%         99.000000      4.268770e+05              24883.000000   \n",
            "75%        228.000000      7.346030e+05              41522.000000   \n",
            "max     147757.000000      8.622467e+06             520467.000000   \n",
            "\n",
            "       5 to 9 years population  10 to 14 years population  \\\n",
            "count             32494.000000               32494.000000   \n",
            "mean              46775.165938               48785.864006   \n",
            "std               86410.304694               92184.448233   \n",
            "min                   0.000000                   0.000000   \n",
            "25%                4626.000000                5129.000000   \n",
            "50%               23220.000000               21482.000000   \n",
            "75%               37677.000000               36449.000000   \n",
            "max              469186.000000              501922.000000   \n",
            "\n",
            "       15 to 19 years population  ...  monthly_listeners  Number of Shows  \\\n",
            "count               32494.000000  ...       4.900000e+04    707628.000000   \n",
            "mean                49577.864529  ...       4.470017e+06         1.240002   \n",
            "std                 86235.886120  ...       1.001371e+07         6.961113   \n",
            "min                     0.000000  ...       1.000000e+00         1.000000   \n",
            "25%                  6303.000000  ...       1.715840e+05         1.000000   \n",
            "50%                 27687.000000  ...       1.121458e+06         1.000000   \n",
            "75%                 41331.000000  ...       3.919792e+06         1.000000   \n",
            "max                467114.000000  ...       2.564413e+08      5707.000000   \n",
            "\n",
            "       Avg. Tickets Sold  Avg. Gross USD  Avg. Event Capacity  \\\n",
            "count      707628.000000    7.076280e+05         7.076270e+05   \n",
            "mean         2118.999729    1.183563e+05         2.961416e+03   \n",
            "std          4085.119298    4.351130e+05         1.605620e+05   \n",
            "min             0.000000    5.000000e-01         1.000000e+01   \n",
            "25%           300.000000    5.550000e+03         5.000000e+02   \n",
            "50%           796.000000    2.108025e+04         1.148000e+03   \n",
            "75%          1927.000000    8.218125e+04         2.500000e+03   \n",
            "max        190200.000000    8.005627e+07         1.350076e+08   \n",
            "\n",
            "       Ticket Price Min USD  Ticket Price Max USD  Ticket Price Avg. USD  \\\n",
            "count         707628.000000          7.076280e+05          707621.000000   \n",
            "mean              26.759060          2.942475e+04              34.498404   \n",
            "std              213.909397          2.470720e+07              71.749407   \n",
            "min                0.000000         -1.495000e+02               0.010000   \n",
            "25%               15.000000          1.500000e+01              16.400000   \n",
            "50%               23.000000          3.000000e+01              26.510000   \n",
            "75%               35.000000          6.000000e+01              44.180000   \n",
            "max           156322.000000          2.078386e+10           44920.000000   \n",
            "\n",
            "               Month    day_of_week  \n",
            "count  707628.000000  707628.000000  \n",
            "mean        6.525454       3.516048  \n",
            "std         3.366555       1.743510  \n",
            "min         1.000000       0.000000  \n",
            "25%         4.000000       2.000000  \n",
            "50%         6.000000       4.000000  \n",
            "75%        10.000000       5.000000  \n",
            "max        12.000000       6.000000  \n",
            "\n",
            "[8 rows x 32 columns]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSummary statistics of the dataset:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCoBIYnAj83o",
        "outputId": "f1c6eb21-280e-464c-84d2-ef4e2b1fec7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values and Percentage:\n",
            "                                 Missing Values  Percentage\n",
            "yt Description                           682831   96.495758\n",
            "yt Channel ID                            675526   95.463436\n",
            "yt Title                                 675526   95.463436\n",
            "yt name                                  675526   95.463436\n",
            "yt View Count                            675526   95.463436\n",
            "yt Subscriber Count                      675526   95.463436\n",
            "yt Published At                          675526   95.463436\n",
            "yt Video Count                           675526   95.463436\n",
            "sp popularity                            675172   95.413409\n",
            "sp artist_name                           675172   95.413409\n",
            "sp followers                             675172   95.413409\n",
            "sp artist_genre                          675172   95.413409\n",
            "Total population                         675134   95.408039\n",
            "10 to 14 years population                675134   95.408039\n",
            "15 to 19 years population                675134   95.408039\n",
            "Under 5 years population                 675134   95.408039\n",
            "5 to 9 years population                  675134   95.408039\n",
            "20 to 24 years population                675134   95.408039\n",
            "25 to 34 years population                675134   95.408039\n",
            "45 to 54 years population                675134   95.408039\n",
            "35 to 44 years population                675134   95.408039\n",
            "65 to 74 years population                675134   95.408039\n",
            "75 to 84 years population                675134   95.408039\n",
            "55 to 59 years population                675134   95.408039\n",
            "60 to 64 years population                675134   95.408039\n",
            "85 years and over population             675134   95.408039\n",
            "Median age                               675134   95.408039\n",
            "monthly_listeners                        658628   93.075458\n",
            "Year                                     597938   84.498918\n",
            "headliner_monthly_listeners              597938   84.498918\n",
            "Support_Total_Monthly_Listeners          597938   84.498918\n",
            "Support                                  371754   52.535230\n",
            "Market                                    33306    4.706710\n",
            "Genre                                     32303    4.564969\n",
            "Promoter                                  15094    2.133042\n",
            "Company Type                               2901    0.409961\n",
            "Venue                                      1047    0.147959\n",
            "State                                        10    0.001413\n",
            "Ticket Price Avg. USD                         7    0.000989\n",
            "Avg. Event Capacity                           1    0.000141\n",
            "Headliner                                     0    0.000000\n",
            "Event Date                                    0    0.000000\n",
            "Number of Shows                               0    0.000000\n",
            "City                                          0    0.000000\n",
            "Country                                       0    0.000000\n",
            "Avg. Tickets Sold                             0    0.000000\n",
            "Currency                                      0    0.000000\n",
            "Avg. Capacity Sold                            0    0.000000\n",
            "Avg. Gross USD                                0    0.000000\n",
            "Ticket Price Min USD                          0    0.000000\n",
            "Ticket Price Max USD                          0    0.000000\n",
            "Month                                         0    0.000000\n",
            "day_of_week                                   0    0.000000\n"
          ]
        }
      ],
      "source": [
        "# Display missing value counts for each column\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "print(\"Missing Values and Percentage:\")\n",
        "print(pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage}).sort_values(by='Percentage', ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFnxeWZyuCTI"
      },
      "source": [
        "Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa-BrvJ7mMic",
        "outputId": "0bdf3dac-e213-4893-f2fb-97cf4efad741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remaining missing values in 'State': 0\n",
            "Remaining missing values in the dataset:\n",
            "Event Date                              0\n",
            "Headliner                               0\n",
            "sp artist_name                     675172\n",
            "sp artist_genre                    675172\n",
            "sp followers                       675172\n",
            "sp popularity                      675172\n",
            "yt name                            675526\n",
            "yt Channel ID                      675526\n",
            "yt Title                           675526\n",
            "yt Description                     682831\n",
            "yt Published At                    675526\n",
            "yt View Count                      675526\n",
            "yt Subscriber Count                675526\n",
            "yt Video Count                     675526\n",
            "Total population                   675134\n",
            "Under 5 years population           675134\n",
            "5 to 9 years population            675134\n",
            "10 to 14 years population          675134\n",
            "15 to 19 years population          675134\n",
            "20 to 24 years population          675134\n",
            "25 to 34 years population          675134\n",
            "35 to 44 years population          675134\n",
            "45 to 54 years population          675134\n",
            "55 to 59 years population          675134\n",
            "60 to 64 years population          675134\n",
            "65 to 74 years population          675134\n",
            "75 to 84 years population          675134\n",
            "85 years and over population       675134\n",
            "Median age                         675134\n",
            "Year                               597938\n",
            "headliner_monthly_listeners        597938\n",
            "Support_Total_Monthly_Listeners    597938\n",
            "monthly_listeners                  658628\n",
            "Number of Shows                         0\n",
            "Support                                 0\n",
            "Venue                                   0\n",
            "City                                    0\n",
            "State                                   0\n",
            "Country                                 0\n",
            "Market                                  0\n",
            "Company Type                            0\n",
            "Currency                                0\n",
            "Promoter                                0\n",
            "Genre                                   0\n",
            "Avg. Tickets Sold                       0\n",
            "Avg. Gross USD                          0\n",
            "Avg. Event Capacity                     0\n",
            "Avg. Capacity Sold                      0\n",
            "Ticket Price Min USD                    0\n",
            "Ticket Price Max USD                    0\n",
            "Ticket Price Avg. USD                   0\n",
            "Month                                   0\n",
            "day_of_week                             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "# Define a function to infer 'State' based on 'City'\n",
        "def fill_state(city):\n",
        "    if pd.isnull(city) or city == \"Unknown\":\n",
        "        return \"Unknown\"\n",
        "    geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
        "    try:\n",
        "        # Attempt to fetch the location information using geopy\n",
        "        location = geolocator.geocode(city, timeout=10)\n",
        "        if location:\n",
        "            # Extract state information from the address\n",
        "            return location.address.split(',')[-3].strip()\n",
        "        else:\n",
        "            return \"Unknown\"\n",
        "    except:\n",
        "        # If geocoding fails, return 'Unknown'\n",
        "        return \"Unknown\"\n",
        "\n",
        "# Handle missing values in the 'State' column using the 'fill_state' function\n",
        "df['State'] = df.apply(lambda x: fill_state(x['City']) if pd.isnull(x['State']) else x['State'], axis=1)\n",
        "\n",
        "# Print to check remaining missing values in the 'State' column\n",
        "print(f\"Remaining missing values in 'State': {df['State'].isnull().sum()}\")\n",
        "\n",
        "# Fill missing values in categorical columns with 'Missing'\n",
        "categorical_cols = ['Support', 'Market', 'Genre', 'Promoter', 'Company Type', 'Venue']\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna(\"Missing\")\n",
        "\n",
        "# Fill missing values in numerical columns with mean\n",
        "df['Ticket Price Avg. USD'] = df['Ticket Price Avg. USD'].fillna(df['Ticket Price Avg. USD'].mean())\n",
        "df['Avg. Event Capacity'] = df['Avg. Event Capacity'].fillna(df['Avg. Event Capacity'].mean())\n",
        "\n",
        "# Check if any missing values remain in the dataset\n",
        "print(\"Remaining missing values in the dataset:\")\n",
        "print(df.isnull().sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "671t55DI8U5d",
        "outputId": "0a850b8f-df89-412c-d0a9-41e6a10968e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                               Headliner  \\\n",
            "0                                  Creed   \n",
            "1                                  Creed   \n",
            "2  Bruce Springsteen & The E Street Band   \n",
            "3                                  Creed   \n",
            "4                Billy Joel, Rod Stewart   \n",
            "\n",
            "                          sp artist_name  \\\n",
            "0                                  Creed   \n",
            "1                                  Creed   \n",
            "2  Bruce Springsteen & The E Street Band   \n",
            "3                                  Creed   \n",
            "4                Billy Joel, Rod Stewart   \n",
            "\n",
            "                                     sp artist_genre  sp followers  \\\n",
            "0  ['alternative metal', 'nu metal', 'post-grunge...     3527070.0   \n",
            "1  ['alternative metal', 'nu metal', 'post-grunge...     3527070.0   \n",
            "2  ['heartland rock', 'mellow gold', 'permanent w...     6567386.0   \n",
            "3  ['alternative metal', 'nu metal', 'post-grunge...     3527070.0   \n",
            "4  ['album rock', 'classic rock', 'mellow gold', ...     6312751.0   \n",
            "\n",
            "   sp popularity                                yt name  \\\n",
            "0           74.0                                  Creed   \n",
            "1           74.0                                  Creed   \n",
            "2           79.0  Bruce Springsteen & The E Street Band   \n",
            "3           74.0                                  Creed   \n",
            "4           79.0                Billy Joel, Rod Stewart   \n",
            "\n",
            "              yt Channel ID           yt Title  \\\n",
            "0  UCP-tFf_VMQzhyeKMONL1KvQ              Creed   \n",
            "1  UCP-tFf_VMQzhyeKMONL1KvQ              Creed   \n",
            "2  UCcu7ANuD9J7hnTQCREqIc4Q  Bruce Springsteen   \n",
            "3  UCP-tFf_VMQzhyeKMONL1KvQ              Creed   \n",
            "4  UC0yYX8_4R-pITDGp0YC_Qfg      Rock Playlist   \n",
            "\n",
            "                                      yt Description  \\\n",
            "0  Subscribe to Creed's Official Youtube Channel ...   \n",
            "1  Subscribe to Creed's Official Youtube Channel ...   \n",
            "2     Bruce Springsteen's official YouTube channel.    \n",
            "3  Subscribe to Creed's Official Youtube Channel ...   \n",
            "4                                                NaN   \n",
            "\n",
            "               yt Published At  ...  Ticket Price Min USD  \\\n",
            "0         2009-03-30T21:30:11Z  ...                  39.5   \n",
            "1         2009-03-30T21:30:11Z  ...                  39.5   \n",
            "2         2006-04-01T18:32:28Z  ...                  49.5   \n",
            "3         2009-03-30T21:30:11Z  ...                  39.5   \n",
            "4  2024-02-28T03:11:06.872221Z  ...                  69.5   \n",
            "\n",
            "   Ticket Price Max USD  Ticket Price Avg. USD  Month  Day  Day_of_Week  \\\n",
            "0                 225.0               0.363094      9   18            2   \n",
            "1                 225.0               0.693551      9   14            5   \n",
            "2                 299.5               1.824159      9   13            4   \n",
            "3                 225.0               0.823170      9   13            4   \n",
            "4                 349.5               2.546260      9   13            4   \n",
            "\n",
            "   Day_of_Year  Is_Weekend  Price_Range  Gross_Per_Capacity  \n",
            "0          262           0    -0.001182            1.249385  \n",
            "1          258           1    -0.001182            2.119206  \n",
            "2          257           0    -0.001180            5.094955  \n",
            "3          257           0    -0.001182            2.460335  \n",
            "4          257           0    -0.001179            6.995812  \n",
            "\n",
            "[5 rows x 57 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Convert 'Event Date' to datetime format and extract features\n",
        "df['Event Date'] = pd.to_datetime(df['Event Date'])\n",
        "\n",
        "# Feature engineering: extract useful time-related features\n",
        "df['Year'] = df['Event Date'].dt.year\n",
        "df['Month'] = df['Event Date'].dt.month\n",
        "df['Day'] = df['Event Date'].dt.day\n",
        "df['Day_of_Week'] = df['Event Date'].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
        "df['Day_of_Year'] = df['Event Date'].dt.dayofyear\n",
        "df['Is_Weekend'] = (df['Day_of_Week'] >= 5).astype(int)  # 1 for Saturday/Sunday\n",
        "\n",
        "# Create interaction features between price and capacity\n",
        "df['Price_Range'] = df['Ticket Price Max USD'] - df['Ticket Price Min USD']\n",
        "\n",
        "# Scale/normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Select numerical columns for normalization\n",
        "numerical_features = [\n",
        "    'Ticket Price Avg. USD',\n",
        "    'Avg. Event Capacity',\n",
        "    'Price_Range',\n",
        "]\n",
        "\n",
        "# Normalize the numerical features\n",
        "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "# Drop the original 'Event Date' column after processing\n",
        "df = df.drop(columns=['Event Date'])\n",
        "df = df.drop(columns=['day_of_week'])\n",
        "# Final check: View the processed data\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K8EoJ6E-svu"
      },
      "source": [
        "Distribution-balanced Stratified Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FLqXXn2-sKy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def dbscv(x: np.array, y: np.array, params: dict) -> (list, list, list, list):\n",
        "    \"\"\"\n",
        "    Distribution-balanced stratified cross-validation (DBSCV) splitting method.\n",
        "\n",
        "    Parameters:\n",
        "    - x: np.array, feature matrix of shape (N, M)\n",
        "    - y: np.array, labels array of shape (N,)\n",
        "    - params: dict, should contain 'K' (number of folds)\n",
        "\n",
        "    Returns:\n",
        "    - x_fit: list of np.array, training feature sets for each fold\n",
        "    - x_val: list of np.array, validation feature sets for each fold\n",
        "    - y_fit: list of np.array, training label sets for each fold\n",
        "    - y_val: list of np.array, validation label sets for each fold\n",
        "    \"\"\"\n",
        "    k = params.get('K', 5)  # Number of folds, default is 5\n",
        "    N, M = x.shape  # N: number of samples, M: number of features\n",
        "    classes = np.unique(y)  # Unique class labels\n",
        "\n",
        "    # Initialize reference feature vector X0 (zeros for continuous attributes)\n",
        "    X0 = np.zeros(M)\n",
        "\n",
        "    # Initialize folds: T[0] corresponds to fold 1\n",
        "    T = [[] for _ in range(k)]\n",
        "\n",
        "    # List to hold remaining samples after main distribution\n",
        "    L_r = []\n",
        "\n",
        "    # For each class, construct the sorted list L_i\n",
        "    for c in classes:\n",
        "        # Indices of samples with class label c\n",
        "        S_i = np.where(y == c)[0].tolist()\n",
        "        Li = []\n",
        "        last_sample = X0\n",
        "\n",
        "        # Step (2): Sort the cases of each class\n",
        "        while S_i:\n",
        "            # Extract samples of class c\n",
        "            samples = x[S_i, :]\n",
        "            # Compute Euclidean distances to the last sample\n",
        "            distances = np.linalg.norm(samples - last_sample, axis=1)\n",
        "            # Find the sample with the minimum distance\n",
        "            min_idx = np.argmin(distances)\n",
        "            sample_index = S_i[min_idx]\n",
        "            # Add to the sorted list Li\n",
        "            Li.append(sample_index)\n",
        "            # Update last_sample and remove the selected sample from S_i\n",
        "            last_sample = x[sample_index]\n",
        "            S_i.pop(min_idx)\n",
        "\n",
        "        # Step (3): Partition each Li into k folds\n",
        "        idx = 0\n",
        "        while idx + k <= len(Li):\n",
        "            for j in range(k):\n",
        "                index = Li[idx + j]\n",
        "                T[j].append(index)\n",
        "            idx += k\n",
        "\n",
        "        # Collect remaining samples to L_r\n",
        "        Li_remain = Li[idx:]\n",
        "        if Li_remain:\n",
        "            L_r.extend(Li_remain)\n",
        "\n",
        "    # Distribute remaining samples in L_r into folds T_j\n",
        "    for i, index in enumerate(L_r):\n",
        "        T[i % k].append(index)\n",
        "\n",
        "    # Prepare training and validation sets for each fold\n",
        "    x_fit = []\n",
        "    x_val = []\n",
        "    y_fit = []\n",
        "    y_val = []\n",
        "\n",
        "    indices_all = np.arange(N)\n",
        "\n",
        "    for j in range(k):\n",
        "        val_indices = np.array(T[j])\n",
        "        train_indices = np.setdiff1d(indices_all, val_indices)\n",
        "\n",
        "        x_val.append(x[val_indices])\n",
        "        y_val.append(y[val_indices])\n",
        "\n",
        "        x_fit.append(x[train_indices])\n",
        "        y_fit.append(y[train_indices])\n",
        "\n",
        "    return x_fit, x_val, y_fit, y_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txtqwsus-56L"
      },
      "source": [
        "Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "SdC4zufy-33o"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "categorical_features = ['Headliner', 'Support', 'Market', 'Genre', 'Promoter', 'Company Type', 'Venue', 'City', 'State']\n",
        "numerical_features = ['Avg. Gross USD', 'Ticket Price Avg. USD', 'Avg. Event Capacity',\n",
        "                      'Price_Range', 'Year', 'Month', 'Day',\n",
        "                      'Day_of_Year', 'Is_Weekend']\n",
        "# Encode categorical features with LabelEncoder\n",
        "label_encoders = {}\n",
        "for col in categorical_features:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le  # Save the encoder for later use\n",
        "\n",
        "# Concatenate encoded categorical features with numerical features\n",
        "final_df = df[numerical_features + categorical_features]\n",
        "\n",
        "# Ensure no missing values remain in the final dataset\n",
        "final_df = final_df.fillna(0)\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = final_df.drop(columns=['Avg. Gross USD']).values  # Drop the target column from features\n",
        "y = final_df['Avg. Gross USD'].values                # Target variable\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Stratified split to maintain distribution\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,       # 20% for final testing\n",
        "    random_state=42,     # Seed for reproducibility\n",
        "    shuffle=True         # Shuffle before splitting\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQQP-99VtKzM",
        "outputId": "9ac15474-983c-4f71-9521-b6d9fd2ec2b8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (566102, 17)\n",
            "Test set shape: (141526, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optuna XGBoost**"
      ],
      "metadata": {
        "id": "kP2Nut_C0FBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf9nSGeYkCHL",
        "outputId": "6dd223b9-c3d7-49c4-c85c-3ff5647e7170"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.37)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "from sklearn.base import RegressorMixin\n",
        "\n",
        "class XGBWrapper(XGBRegressor, RegressorMixin):\n",
        "    pass\n",
        "# ===============================\n",
        "# Example: Optuna for XGBoost\n",
        "# Minimizing MAE via cross-validation\n",
        "# ===============================\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"\n",
        "    Objective function for Optuna.\n",
        "    We define the hyperparameter search space\n",
        "    and train an XGBoost model using those hyperparameters.\n",
        "\n",
        "    The function returns the cross-validated MAE (Mean Absolute Error).\n",
        "    \"\"\"\n",
        "    # 1) Define the hyperparameter search space\n",
        "    param = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0, step=0.1),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0, step=0.1),\n",
        "        # Feel free to add more hyperparameters if needed\n",
        "    }\n",
        "\n",
        "    # 2) Initialize the XGBoost model\n",
        "    model = XGBRegressor(\n",
        "        objective='reg:squarederror',\n",
        "        random_state=42,\n",
        "        **param\n",
        "    )\n",
        "\n",
        "    # 3) Create a 5-fold cross-validation\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # 4) Perform cross-validation using negative MAE\n",
        "    #    (since sklearn uses \"maximize\" by default, we negate MAE)\n",
        "    scores = cross_val_score(\n",
        "        model,\n",
        "        X_train,\n",
        "        y_train,\n",
        "        scoring='neg_mean_absolute_error',\n",
        "        cv=kf,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # 5) Convert negative MAE to positive, then compute average\n",
        "    avg_mae = -scores.mean()\n",
        "\n",
        "    # 6) Return the metric that needs to be minimized\n",
        "    return avg_mae\n",
        "\n",
        "\n",
        "# Create a study object.\n",
        "# \"direction='minimize'\" means we want to minimize the objective.\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=TPESampler(seed=42))\n",
        "\n",
        "# Start optimization.\n",
        "# \"n_trials=20\" means we will try 20 different hyperparameter sets.\n",
        "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
        "\n",
        "print(\"\\n[Optuna] Best trial object:\")\n",
        "print(study.best_trial)\n",
        "\n",
        "print(\"\\n[Optuna] Best params found:\")\n",
        "print(study.best_params)\n",
        "\n",
        "# ===============================\n",
        "# Train an XGBoost model with the best hyperparameters\n",
        "# ===============================\n",
        "\n",
        "best_params = study.best_params\n",
        "optuna_xgb = XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    random_state=42,\n",
        "    **best_params\n",
        ")\n",
        "\n",
        "# Fit the model using the entire training set\n",
        "optuna_xgb.fit(X_train, y_train)\n",
        "\n",
        "# ===============================\n",
        "# Evaluate on the test set\n",
        "# ===============================\n",
        "y_pred_optuna = optuna_xgb.predict(X_test)\n",
        "\n",
        "mae_optuna = mean_absolute_error(y_test, y_pred_optuna)\n",
        "mse_optuna = mean_squared_error(y_test, y_pred_optuna)\n",
        "r2_optuna  = r2_score(y_test, y_pred_optuna)\n",
        "\n",
        "print(\"\\n[Optuna XGBoost] Test set performance:\")\n",
        "print(f\"MAE: {mae_optuna:.4f}\")\n",
        "print(f\"MSE: {mse_optuna:.4f}\")\n",
        "print(f\"R² : {r2_optuna:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641,
          "referenced_widgets": [
            "e22775fb0cd34a8b9fb762f38acc702e",
            "32a8f6a4cb32406d90078598f4ab6f1e",
            "91fde5da00054e4db5aa731f773f87d4",
            "7175c0738dca45d486e874602ef79f1b",
            "a7f7162843f34784b30f39bcbb15c818",
            "2fcfa4abc8a846718f1f339bcc7c0aef",
            "1634abbdac7a4be4bef2a03ed6bbfd32",
            "3bbfbaab38e34abb903dfb30a9c1b746",
            "1c5b091f5fec4999b420a66f98f191bd",
            "fad0de1fd1b745a984af5e75568013e4",
            "8ab01a1022074ee1bba7c96cf5235a5e"
          ]
        },
        "id": "6MMxOQlukEfj",
        "outputId": "a8259c66-d21c-4f7f-e200-0e1922ef6778"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-14 06:17:36,725] A new study created in memory with name: no-name-41fbb064-dffb-4e86-901d-b2e1a34627c8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e22775fb0cd34a8b9fb762f38acc702e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[I 2025-02-14 06:17:53,299] Trial 0 finished with value: 22700.820290350177 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.06504856968981275, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:17:57,898] Trial 1 finished with value: 29487.54818589813 and parameters: {'n_estimators': 89, 'max_depth': 3, 'learning_rate': 0.13983740016490973, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:18:05,176] Trial 2 finished with value: 23906.321706625313 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.11536162338241392, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:18:10,472] Trial 3 finished with value: 43183.11504582285 and parameters: {'n_estimators': 96, 'max_depth': 5, 'learning_rate': 0.0199473547030745, 'subsample': 0.8, 'colsample_bytree': 0.7}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:18:18,646] Trial 4 finished with value: 67070.71143793297 and parameters: {'n_estimators': 203, 'max_depth': 4, 'learning_rate': 0.005292705365436975, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:18:27,658] Trial 5 finished with value: 31724.17487459335 and parameters: {'n_estimators': 247, 'max_depth': 4, 'learning_rate': 0.018785426399210624, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:18:34,497] Trial 6 finished with value: 117040.4792047155 and parameters: {'n_estimators': 202, 'max_depth': 4, 'learning_rate': 0.0014492412389916862, 'subsample': 1.0, 'colsample_bytree': 1.0}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:18:45,736] Trial 7 finished with value: 102983.25049697081 and parameters: {'n_estimators': 252, 'max_depth': 5, 'learning_rate': 0.0017456037635797405, 'subsample': 0.9, 'colsample_bytree': 0.8}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:18:51,082] Trial 8 finished with value: 138635.96049515932 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.0012167028814593455, 'subsample': 1.0, 'colsample_bytree': 0.7}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:19:00,721] Trial 9 finished with value: 29713.131083902183 and parameters: {'n_estimators': 216, 'max_depth': 5, 'learning_rate': 0.01942099825171803, 'subsample': 0.8, 'colsample_bytree': 0.6}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:19:16,651] Trial 10 finished with value: 22962.69564297645 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.053011269715584376, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:19:32,402] Trial 11 finished with value: 22923.151796907667 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.058218083468221736, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:19:43,605] Trial 12 finished with value: 23997.561754523398 and parameters: {'n_estimators': 144, 'max_depth': 8, 'learning_rate': 0.04818698083227556, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:19:54,223] Trial 13 finished with value: 24066.162531724658 and parameters: {'n_estimators': 154, 'max_depth': 8, 'learning_rate': 0.2944295666057774, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:20:05,235] Trial 14 finished with value: 23813.598752923153 and parameters: {'n_estimators': 114, 'max_depth': 9, 'learning_rate': 0.05260048573838175, 'subsample': 0.6, 'colsample_bytree': 0.7}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:20:29,379] Trial 15 finished with value: 34140.58224591159 and parameters: {'n_estimators': 292, 'max_depth': 8, 'learning_rate': 0.007065703402403497, 'subsample': 0.9, 'colsample_bytree': 0.9}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:20:43,954] Trial 16 finished with value: 22719.629812455554 and parameters: {'n_estimators': 174, 'max_depth': 9, 'learning_rate': 0.10552025486201068, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:20:58,544] Trial 17 finished with value: 23486.27995715758 and parameters: {'n_estimators': 179, 'max_depth': 9, 'learning_rate': 0.2461836147383402, 'subsample': 0.7, 'colsample_bytree': 0.6}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:21:05,967] Trial 18 finished with value: 23253.660207457706 and parameters: {'n_estimators': 113, 'max_depth': 7, 'learning_rate': 0.11097099466719954, 'subsample': 0.8, 'colsample_bytree': 0.9}. Best is trial 0 with value: 22700.820290350177.\n",
            "[I 2025-02-14 06:21:23,576] Trial 19 finished with value: 22774.85857581784 and parameters: {'n_estimators': 187, 'max_depth': 9, 'learning_rate': 0.031104325874865335, 'subsample': 0.7, 'colsample_bytree': 0.8}. Best is trial 0 with value: 22700.820290350177.\n",
            "\n",
            "[Optuna] Best trial object:\n",
            "FrozenTrial(number=0, state=1, values=[22700.820290350177], datetime_start=datetime.datetime(2025, 2, 14, 6, 17, 36, 744278), datetime_complete=datetime.datetime(2025, 2, 14, 6, 17, 53, 299145), params={'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.06504856968981275, 'subsample': 0.8, 'colsample_bytree': 0.6}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_estimators': IntDistribution(high=300, log=False, low=50, step=1), 'max_depth': IntDistribution(high=10, log=False, low=3, step=1), 'learning_rate': FloatDistribution(high=0.3, log=True, low=0.001, step=None), 'subsample': FloatDistribution(high=1.0, log=False, low=0.6, step=0.1), 'colsample_bytree': FloatDistribution(high=1.0, log=False, low=0.6, step=0.1)}, trial_id=0, value=None)\n",
            "\n",
            "[Optuna] Best params found:\n",
            "{'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.06504856968981275, 'subsample': 0.8, 'colsample_bytree': 0.6}\n",
            "\n",
            "[Optuna XGBoost] Test set performance:\n",
            "MAE: 22036.7280\n",
            "MSE: 9103889000.8416\n",
            "R² : 0.9471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stacking Model**"
      ],
      "metadata": {
        "id": "EX-CKjA30L1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# Initialize containers for out-of-fold predictions\n",
        "meta_features_train = np.zeros((X_train.shape[0], 2))  # RF and XGB columns\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"\\nGenerating meta-features through 5-fold cross-validation:\")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n",
        "    print(f\"\\nProcessing Fold {fold+1}/5\")\n",
        "\n",
        "    # Split training data into training/validation subsets\n",
        "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
        "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "    # ----- Random Forest Training -----\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=150,\n",
        "        max_depth=10,\n",
        "        min_samples_split=5,\n",
        "        n_jobs=-1,              # Use all cores\n",
        "        random_state=42\n",
        "    )\n",
        "    rf.fit(X_tr, y_tr)\n",
        "    meta_features_train[val_idx, 0] = rf.predict(X_val)\n",
        "\n",
        "    # ----- XGBoost Training -----\n",
        "    xgb = XGBRegressor(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=6,\n",
        "        subsample=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "    xgb.fit(X_tr, y_tr)\n",
        "    meta_features_train[val_idx, 1] = xgb.predict(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK_XSJ7FtMJO",
        "outputId": "6cf70e05-3d4f-48dd-a22d-f1083814019f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating meta-features through 5-fold cross-validation:\n",
            "\n",
            "Processing Fold 1/5\n",
            "\n",
            "Processing Fold 2/5\n",
            "\n",
            "Processing Fold 3/5\n",
            "\n",
            "Processing Fold 4/5\n",
            "\n",
            "Processing Fold 5/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================\n",
        "#  Meta-Model Training (ElasticNet)\n",
        "# ==================================\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Initialize and train meta-model\n",
        "meta_model = ElasticNet(\n",
        "    alpha=0.001,        # Regularization strength\n",
        "    l1_ratio=0.7,       # Balance between L1/L2\n",
        "    max_iter=10000,     # Ensure convergence\n",
        "    random_state=42\n",
        ")\n",
        "meta_model.fit(meta_features_train, y_train)\n",
        "\n",
        "# Evaluate on training meta-features\n",
        "train_pred = meta_model.predict(meta_features_train)\n",
        "print(\"\\nMeta-model training performance:\")\n",
        "print(f\"- MSE: {mean_squared_error(y_train, train_pred):.4f}\")\n",
        "print(f\"- R²: {r2_score(y_train, train_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33D8Yoq2tPjP",
        "outputId": "9e5ca897-e990-411a-cc27-372d8d62151d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Meta-model training performance:\n",
            "- MSE: 10444581972.6508\n",
            "- R²: 0.9460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================\n",
        "#  Final Evaluation on Test Set\n",
        "# ===================================\n",
        "# Retrain base models on full training data\n",
        "print(\"\\nRetraining base models on full training set...\")\n",
        "\n",
        "# Random Forest\n",
        "final_rf = RandomForestRegressor(\n",
        "    n_estimators=150,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ").fit(X_train, y_train)\n",
        "\n",
        "# XGBoost\n",
        "final_xgb = XGBRegressor(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ").fit(X_train, y_train)\n",
        "\n",
        "# Generate test set meta-features\n",
        "meta_features_test = np.column_stack([\n",
        "    final_rf.predict(X_test),\n",
        "    final_xgb.predict(X_test)\n",
        "])\n",
        "\n",
        "# Final prediction and evaluation\n",
        "test_pred = meta_model.predict(meta_features_test)\n",
        "print(\"\\nFinal test set performance:\")\n",
        "print(f\"- MSE: {mean_squared_error(y_test, test_pred):.4f}\")\n",
        "print(f\"- R²: {r2_score(y_test, test_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIhG3dYXtTEB",
        "outputId": "ec1884b9-1456-4c61-f7f9-ca40ae9af1a8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Retraining base models on full training set...\n",
            "\n",
            "Final test set performance:\n",
            "- MSE: 6565166562.3304\n",
            "- R²: 0.9619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UeiJMi5Atxu"
      },
      "outputs": [],
      "source": [
        "# Parameters for DBSCV\n",
        "params = {'K': 5}  # Number of folds\n",
        "\n",
        "# Apply DBSCV to the dataset\n",
        "x_fit, x_val, y_fit, y_val = dbscv(X_train, y_train, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "596WKu7TDpad",
        "outputId": "aba26232-8175-46eb-ed14-9e84b8336369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest - Fold 1: MSE = 0.0447, R^2 = 0.9537\n",
            "Random Forest - Fold 2: MSE = 0.0201, R^2 = 0.9802\n",
            "Random Forest - Fold 3: MSE = 0.0694, R^2 = 0.9397\n",
            "Random Forest - Fold 4: MSE = 0.0056, R^2 = 0.9939\n",
            "Random Forest - Fold 5: MSE = 0.0031, R^2 = 0.9968\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Initialize Random Forest\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train and evaluate using DBSCV splits\n",
        "rf_scores = []  # To store evaluation metrics for each fold\n",
        "\n",
        "for i in range(len(x_fit)):\n",
        "    # Train the model on training set\n",
        "    rf_model.fit(x_fit[i], y_fit[i])\n",
        "\n",
        "    # Predict on validation set\n",
        "    predictions = rf_model.predict(x_val[i])\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_val[i], predictions)\n",
        "    r2 = r2_score(y_val[i], predictions)\n",
        "    rf_scores.append((mse, r2))\n",
        "\n",
        "# Print evaluation results for each fold\n",
        "for fold, (mse, r2) in enumerate(rf_scores, 1):\n",
        "    print(f\"Random Forest - Fold {fold}: MSE = {mse:.4f}, R^2 = {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNz09OK9DrHK",
        "outputId": "5919c45f-fe48-4143-bf9e-5199ec189b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost - Fold 1: MSE = 16559011190.2853, R^2 = 0.9162\n",
            "XGBoost - Fold 2: MSE = 39192167325.5710, R^2 = 0.8278\n",
            "XGBoost - Fold 3: MSE = 11240037901.6352, R^2 = 0.9360\n",
            "XGBoost - Fold 4: MSE = 11700924396.8403, R^2 = 0.9350\n",
            "XGBoost - Fold 5: MSE = 16185369833.5226, R^2 = 0.9135\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Initialize XGBoost\n",
        "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, objective='reg:squarederror')\n",
        "\n",
        "# Train and evaluate using DBSCV splits\n",
        "xgb_scores = []  # To store evaluation metrics for each fold\n",
        "\n",
        "for i in range(len(x_fit)):\n",
        "    # Train the model on training set\n",
        "    xgb_model.fit(x_fit[i], y_fit[i])\n",
        "\n",
        "    # Predict on validation set\n",
        "    predictions = xgb_model.predict(x_val[i])\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_val[i], predictions)\n",
        "    r2 = r2_score(y_val[i], predictions)\n",
        "    xgb_scores.append((mse, r2))\n",
        "\n",
        "# Print evaluation results for each fold\n",
        "for fold, (mse, r2) in enumerate(xgb_scores, 1):\n",
        "    print(f\"XGBoost - Fold {fold}: MSE = {mse:.4f}, R^2 = {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aac5F3xRY3TK",
        "outputId": "98e769c3-b735-4ee3-a252-0702359862ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error (RMSE): 0.2159\n"
          ]
        }
      ],
      "source": [
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dbscv2(x: np.array, y: np.array, params: dict) -> list:\n",
        "    k = params.get('K', 5)\n",
        "    N, M = x.shape\n",
        "    classes = np.unique(y)\n",
        "    X0 = np.zeros(M)\n",
        "    T = [[] for _ in range(k)]\n",
        "    L_r = []\n",
        "\n",
        "    for c in classes:\n",
        "        S_i = np.where(y == c)[0].tolist()\n",
        "        Li = []\n",
        "        last_sample = X0\n",
        "\n",
        "        while S_i:\n",
        "            samples = x[S_i]\n",
        "            distances = np.linalg.norm(samples - last_sample, axis=1)\n",
        "            min_idx = np.argmin(distances)\n",
        "            sample_idx = S_i.pop(min_idx)\n",
        "            Li.append(sample_idx)\n",
        "            last_sample = x[sample_idx]\n",
        "\n",
        "        idx = 0\n",
        "        while idx + k <= len(Li):\n",
        "            for j in range(k):\n",
        "                T[j].append(Li[idx + j])\n",
        "            idx += k\n",
        "        L_r.extend(Li[idx:])\n",
        "\n",
        "    for i, idx in enumerate(L_r):\n",
        "        T[i % k].append(idx)\n",
        "\n",
        "    cv_folds = []\n",
        "    all_indices = np.arange(N)\n",
        "    for j in range(k):\n",
        "        val_indices = np.array(T[j])\n",
        "        train_indices = np.setdiff1d(all_indices, val_indices)\n",
        "        cv_folds.append((train_indices, val_indices))\n",
        "\n",
        "    return cv_folds"
      ],
      "metadata": {
        "id": "NRUShehhNUZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn xgboost lightgbm dask[dataframe]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmsJMZE9N6mS",
        "outputId": "a57cf4ef-59c4-44b0-bad2-e95542866af7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Collecting xgboost\n",
            "  Downloading xgboost-2.1.4-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.11/dist-packages (2024.10.0)\n",
            "Collecting dask[dataframe]\n",
            "  Downloading dask-2025.2.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (8.6.1)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (17.0.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask[dataframe]) (3.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[dataframe]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[dataframe]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[dataframe]) (2025.1)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[dataframe]) (1.17.0)\n",
            "Downloading xgboost-2.1.4-py3-none-manylinux_2_28_x86_64.whl (223.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dask-2025.2.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xgboost, dask\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.1.3\n",
            "    Uninstalling xgboost-2.1.3:\n",
            "      Successfully uninstalled xgboost-2.1.3\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2024.10.0\n",
            "    Uninstalling dask-2024.10.0:\n",
            "      Successfully uninstalled dask-2024.10.0\n",
            "Successfully installed dask-2025.2.0 xgboost-2.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GridSearchCV for XGBoost and LightGBM**"
      ],
      "metadata": {
        "id": "N6rQzCag0TNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Define the parameter grids\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 6],\n",
        "    'learning_rate': [0.01, 0.1]\n",
        "}\n",
        "\n",
        "param_grid_lgb = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 6],\n",
        "    'learning_rate': [0.01, 0.1]\n",
        "}\n",
        "\n",
        "# Initialize the models\n",
        "xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "lgb = LGBMRegressor(random_state=42)\n",
        "\n",
        "# Perform GridSearchCV for XGBoost\n",
        "grid_xgb = GridSearchCV(\n",
        "    xgb,\n",
        "    param_grid_xgb,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    n_jobs=-1,\n",
        "    error_score='raise'\n",
        ")\n",
        "grid_xgb.fit(X_train, y_train)\n",
        "\n",
        "grid_lgb = GridSearchCV(\n",
        "    lgb,\n",
        "    param_grid_lgb,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    n_jobs=-1,\n",
        "    error_score='raise'\n",
        ")\n",
        "grid_lgb.fit(X_train, y_train)\n",
        "\n",
        "# Get the best estimators\n",
        "best_xgb = grid_xgb.best_estimator_\n",
        "best_lgb = grid_lgb.best_estimator_\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best XGBoost parameters:\", grid_xgb.best_params_)\n",
        "print(\"Best LightGBM parameters:\", grid_lgb.best_params_)\n",
        "\n",
        "# Evaluate the best XGBoost model on the test set\n",
        "y_pred_xgb = best_xgb.predict(X_test)\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "# Evaluate the best LightGBM model on the test set\n",
        "y_pred_lgb = best_lgb.predict(X_test)\n",
        "mae_lgb = mean_absolute_error(y_test, y_pred_lgb)\n",
        "r2_lgb = r2_score(y_test, y_pred_lgb)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(\"\\nBest XGBoost model:\")\n",
        "print(f\"- MAE: {mae_xgb:.4f}\")\n",
        "print(f\"- R²: {r2_xgb:.4f}\")\n",
        "\n",
        "print(\"\\nBest LightGBM model:\")\n",
        "print(f\"- MAE: {mae_lgb:.4f}\")\n",
        "print(f\"- R²: {r2_lgb:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYRYXlHWPStq",
        "outputId": "765972b0-f921-42b1-dcfe-ea75f331ab93"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031356 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2767\n",
            "[LightGBM] [Info] Number of data points in the train set: 566102, number of used features: 17\n",
            "[LightGBM] [Info] Start training from score 118358.566623\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Best XGBoost parameters: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200}\n",
            "Best LightGBM parameters: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best XGBoost model:\n",
            "- MAE: 22217.5524\n",
            "- R²: 0.9313\n",
            "\n",
            "Best LightGBM model:\n",
            "- MAE: 23452.1467\n",
            "- R²: 0.9273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "model_paths = {\n",
        "    \"optuna_xgb.pkl\": \"/content/optuna_xgb.pkl\",\n",
        "    \"meta_model.pkl\": \"/content/meta_model.pkl\",\n",
        "    \"grid_lgb.pkl\": \"/content/grid_lgb.pkl\",\n",
        "    \"grid_xgb.pkl\": \"/content/grid_xgb.pkl\",\n",
        "    \"final_rf.pkl\": \"/content/final_rf.pkl\",\n",
        "    \"final_xgb.pkl\": \"/content/final_xgb.pkl\"\n",
        "}\n",
        "# Define the trained models to be saved\n",
        "model_objects = {\n",
        "    \"optuna_xgb.pkl\": optuna_xgb,  # Optuna-tuned XGBoost model\n",
        "    \"meta_model.pkl\": meta_model,  # Stacking meta-model (ElasticNet)\n",
        "    \"grid_lgb.pkl\": grid_lgb.best_estimator_,  # Best LightGBM model from GridSearchCV\n",
        "    \"grid_xgb.pkl\": grid_xgb.best_estimator_ ,  # Best XGBoost model from GridSearchCV\n",
        "    \"final_rf.pkl\": final_rf,  # Final Random Forest model\n",
        "    \"final_xgb.pkl\": final_xgb   # Final XGBoost model\n",
        "}\n",
        "\n",
        "for filename, model in model_objects.items():\n",
        "    with open(model_paths[filename], \"wb\") as file:\n",
        "        pickle.dump(model, file)\n",
        "\n",
        "# Print and return the model file paths\n",
        "print(\"✅ Models have been successfully saved at the following locations:\")\n",
        "for name, path in model_paths.items():\n",
        "    print(f\"{name}: {path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdmKJ_0XyExF",
        "outputId": "220bc5c6-5584-452a-dd76-08beacd4cc5f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Models have been successfully saved at the following locations:\n",
            "optuna_xgb.pkl: /content/optuna_xgb.pkl\n",
            "meta_model.pkl: /content/meta_model.pkl\n",
            "grid_lgb.pkl: /content/grid_lgb.pkl\n",
            "grid_xgb.pkl: /content/grid_xgb.pkl\n",
            "final_rf.pkl: /content/final_rf.pkl\n",
            "final_xgb.pkl: /content/final_xgb.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load test dataset\n",
        "test_file_path = '/test_all_genre.csv'\n",
        "test_df = pd.read_csv(test_file_path)\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8cZalAk-ddd",
        "outputId": "9c839f99-afb6-43cb-8692-add94e4d81b5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the dataset:\n",
            "   Event Date  Number of Shows  \\\n",
            "0  2025-01-11                1   \n",
            "1  2025-01-10                1   \n",
            "2  2025-01-10                1   \n",
            "3  2025-01-06                1   \n",
            "4  2024-12-31                1   \n",
            "\n",
            "                                           Headliner                  Support  \\\n",
            "0                                      Buddha Trixie  Sports Coach, Herr God.   \n",
            "1                                       Bonnie Hayes                      NaN   \n",
            "2  The Drifters, Cornell Gunter's Coasters, The P...                      NaN   \n",
            "3                     Gary Lucas & Gods And Monsters                      NaN   \n",
            "4                                 Straight No Chaser                      NaN   \n",
            "\n",
            "                           Venue             City       State        Country  \\\n",
            "0     McMenamins Mission Theater         Portland      Oregon  United States   \n",
            "1          Sweetwater Music Hall      Mill Valley  California  United States   \n",
            "2  Bergen Performing Arts Center        Englewood  New Jersey  United States   \n",
            "3     The Loft @ City Winery NYC         New York    New York  United States   \n",
            "4  Kravis Center – Dreyfoos Hall  West Palm Beach     Florida  United States   \n",
            "\n",
            "                           Market          Company Type  ...  \\\n",
            "0                   Portland, OR   Auditorium / Theatre  ...   \n",
            "1  San Francisco-Oakland-San Jose                  Club  ...   \n",
            "2                        New York  Auditorium / Theatre  ...   \n",
            "3                        New York                  Club  ...   \n",
            "4     West Palm Beach-Fort Pierce  Auditorium / Theatre  ...   \n",
            "\n",
            "  65 to 74 years population 75 to 84 years population  \\\n",
            "0                       NaN                       NaN   \n",
            "1                       NaN                       NaN   \n",
            "2                       NaN                       NaN   \n",
            "3                       NaN                       NaN   \n",
            "4                       NaN                       NaN   \n",
            "\n",
            "  85 years and over population Median age headliner_monthly_listeners  \\\n",
            "0                          NaN        NaN                         NaN   \n",
            "1                          NaN        NaN                         NaN   \n",
            "2                          NaN        NaN                         NaN   \n",
            "3                          NaN        NaN                         NaN   \n",
            "4                          NaN        NaN                    111794.0   \n",
            "\n",
            "  Support_Total_Monthly_Listeners  monthly_listeners  Year  Month day_of_week  \n",
            "0                             NaN                NaN  2025      1           5  \n",
            "1                             NaN                NaN  2025      1           4  \n",
            "2                             NaN                NaN  2025      1           4  \n",
            "3                             NaN                NaN  2025      1           0  \n",
            "4                             0.0           111794.0  2024     12           1  \n",
            "\n",
            "[5 rows x 52 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display missing values before processing\n",
        "missing_values_before = test_df.isnull().sum()\n",
        "missing_percentage_before = (missing_values_before / len(test_df)) * 100\n",
        "print(\"🔍 Missing Values Before Processing:\")\n",
        "print(pd.DataFrame({'Missing Values': missing_values_before, 'Percentage': missing_percentage_before}).sort_values(by='Percentage', ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBaZA1B5-1iK",
        "outputId": "86d565bb-3e28-4271-b5d4-5d8de1d354e4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Missing Values Before Processing:\n",
            "                                 Missing Values  Percentage\n",
            "yt Description                             2729   71.759138\n",
            "yt Title                                   2502   65.790166\n",
            "yt name                                    2502   65.790166\n",
            "yt Channel ID                              2502   65.790166\n",
            "yt View Count                              2502   65.790166\n",
            "yt Subscriber Count                        2502   65.790166\n",
            "yt Video Count                             2502   65.790166\n",
            "yt Published At                            2502   65.790166\n",
            "Total population                           2501   65.763871\n",
            "sp popularity                              2501   65.763871\n",
            "sp artist_genre                            2501   65.763871\n",
            "sp followers                               2501   65.763871\n",
            "15 to 19 years population                  2501   65.763871\n",
            "10 to 14 years population                  2501   65.763871\n",
            "5 to 9 years population                    2501   65.763871\n",
            "Under 5 years population                   2501   65.763871\n",
            "sp artist_name                             2501   65.763871\n",
            "35 to 44 years population                  2501   65.763871\n",
            "25 to 34 years population                  2501   65.763871\n",
            "20 to 24 years population                  2501   65.763871\n",
            "85 years and over population               2501   65.763871\n",
            "Median age                                 2501   65.763871\n",
            "65 to 74 years population                  2501   65.763871\n",
            "45 to 54 years population                  2501   65.763871\n",
            "55 to 59 years population                  2501   65.763871\n",
            "60 to 64 years population                  2501   65.763871\n",
            "75 to 84 years population                  2501   65.763871\n",
            "Support                                    2205   57.980542\n",
            "monthly_listeners                          1491   39.205890\n",
            "Market                                     1279   33.631344\n",
            "Support_Total_Monthly_Listeners            1107   29.108598\n",
            "headliner_monthly_listeners                1107   29.108598\n",
            "State                                      1053   27.688667\n",
            "Event Date                                    0    0.000000\n",
            "Number of Shows                               0    0.000000\n",
            "Headliner                                     0    0.000000\n",
            "Ticket Price Avg. USD                         0    0.000000\n",
            "Ticket Price Max USD                          0    0.000000\n",
            "Ticket Price Min USD                          0    0.000000\n",
            "Avg. Capacity Sold                            0    0.000000\n",
            "Avg. Tickets Sold                             0    0.000000\n",
            "Avg. Event Capacity                           0    0.000000\n",
            "Genre                                         0    0.000000\n",
            "Promoter                                      0    0.000000\n",
            "Venue                                         0    0.000000\n",
            "Currency                                      0    0.000000\n",
            "Company Type                                  0    0.000000\n",
            "Country                                       0    0.000000\n",
            "City                                          0    0.000000\n",
            "Year                                          0    0.000000\n",
            "Month                                         0    0.000000\n",
            "day_of_week                                   0    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Event Date' to datetime format\n",
        "test_df['Event Date'] = pd.to_datetime(test_df['Event Date'])\n",
        "\n",
        "# Extract time-based features\n",
        "test_df['Year'] = test_df['Event Date'].dt.year\n",
        "test_df['Month'] = test_df['Event Date'].dt.month\n",
        "test_df['Day'] = test_df['Event Date'].dt.day\n",
        "test_df['Day_of_Year'] = test_df['Event Date'].dt.dayofyear\n",
        "test_df['Is_Weekend'] = (test_df['Event Date'].dt.dayofweek >= 5).astype(int)  # 1 for Sat/Sun\n",
        "\n",
        "# Create 'Price_Range' feature\n",
        "test_df['Price_Range'] = test_df['Ticket Price Max USD'] - test_df['Ticket Price Min USD']\n",
        "\n"
      ],
      "metadata": {
        "id": "c2mqciOIAgDP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values in categorical features with \"Missing\"\n",
        "categorical_features = ['Headliner', 'Support', 'Market', 'Genre', 'Promoter', 'Company Type', 'Venue', 'City', 'State']\n",
        "for col in categorical_features:\n",
        "    test_df[col] = test_df[col].fillna(\"Missing\")\n",
        "\n",
        "# Fill missing values in numerical features with the column mean\n",
        "numerical_features = ['Ticket Price Avg. USD', 'Avg. Event Capacity', 'Price_Range', 'Year', 'Month', 'Day', 'Day_of_Year', 'Is_Weekend']\n",
        "for col in numerical_features:\n",
        "    test_df[col] = test_df[col].fillna(test_df[col].mean())\n"
      ],
      "metadata": {
        "id": "xcYyduOBAkiL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply StandardScaler (ensure same transformation as training)\n",
        "scaler = StandardScaler()\n",
        "test_df[numerical_features] = scaler.fit_transform(test_df[numerical_features])\n"
      ],
      "metadata": {
        "id": "WHm3RMXsBiJV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in categorical_features:\n",
        "    if col in label_encoders:\n",
        "        le = label_encoders[col]  # Use the pre-fitted encoder from training\n",
        "    else:\n",
        "        le = LabelEncoder()\n",
        "        le.fit(test_df[col].astype(str))  # Fit on test data if missing\n",
        "\n",
        "    # Transform the test dataset, mapping unseen labels to -1\n",
        "    test_df[col] = test_df[col].astype(str).apply(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
        "\n",
        "    # Store the encoder\n",
        "    label_encoders[col] = le\n"
      ],
      "metadata": {
        "id": "on782MYHBliJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define final feature set used in training\n",
        "final_features = numerical_features + categorical_features\n",
        "X_test_final = test_df[final_features].values\n",
        "# Save the processed test dataset\n",
        "processed_test_file = \"/content/processed_test_dataset.csv\"\n",
        "test_df.to_csv(processed_test_file, index=False)\n",
        "\n",
        "print(f\"✅ Processed test dataset saved at: {processed_test_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_2TCoXaFkzP",
        "outputId": "dda9dabf-0929-43c5-d1de-0229c4aea0ea"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Processed test dataset saved at: /content/processed_test_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained models\n",
        "models = {}\n",
        "for name, path in model_paths.items():\n",
        "    with open(path, \"rb\") as file:\n",
        "        models[name] = pickle.load(file)\n",
        "\n",
        "# Load the processed test dataset\n",
        "test_file_path = \"/content/processed_test_dataset.csv\"\n",
        "test_df = pd.read_csv(test_file_path)\n",
        "\n",
        "# Extract feature set for prediction\n",
        "X_test_final = test_df[numerical_features + categorical_features].values\n",
        "\n",
        "# Generate predictions for each individual model\n",
        "predictions = {\n",
        "    \"grid_lgb\": models[\"grid_lgb\"].predict(X_test_final),\n",
        "    \"grid_xgb\": models[\"grid_xgb\"].predict(X_test_final),\n",
        "    \"optuna_xgb\": models[\"optuna_xgb\"].predict(X_test_final),\n",
        "    \"final_rf\": models[\"final_rf\"].predict(X_test_final),\n",
        "    \"final_xgb\": models[\"final_xgb\"].predict(X_test_final)\n",
        "}\n",
        "\n",
        "# Generate meta-features for stacking model (using final_rf and final_xgb)\n",
        "meta_features_test = np.column_stack([\n",
        "    predictions[\"final_rf\"],  # Correct RandomForest predictions\n",
        "    predictions[\"final_xgb\"]  # Correct XGBoost predictions\n",
        "])\n",
        "\n",
        "# Use meta-model to predict final output (stacking)\n",
        "predictions[\"meta_model\"] = models[\"meta_model\"].predict(meta_features_test)\n",
        "\n",
        "# Generate submission CSV files for each model\n",
        "submission_files = {}\n",
        "for name, preds in predictions.items():\n",
        "    submission_df = pd.DataFrame({\"id\": test_df.index, \"Average Gross\": preds})\n",
        "    submission_path = f\"/mnt/data/submission_{name}.csv\"\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "    submission_files[name] = submission_path\n",
        "\n",
        "# Return paths to submission files\n",
        "submission_files\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "fo1QhcIYGKGm",
        "outputId": "047920d2-fccf-4961-c236-1e05de59abe8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['Avg. Gross USD'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-29783e7c3034>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Extract feature set for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mX_test_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumerical_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Generate predictions for each individual model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Avg. Gross USD'] not in index\""
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e22775fb0cd34a8b9fb762f38acc702e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32a8f6a4cb32406d90078598f4ab6f1e",
              "IPY_MODEL_91fde5da00054e4db5aa731f773f87d4",
              "IPY_MODEL_7175c0738dca45d486e874602ef79f1b"
            ],
            "layout": "IPY_MODEL_a7f7162843f34784b30f39bcbb15c818"
          }
        },
        "32a8f6a4cb32406d90078598f4ab6f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fcfa4abc8a846718f1f339bcc7c0aef",
            "placeholder": "​",
            "style": "IPY_MODEL_1634abbdac7a4be4bef2a03ed6bbfd32",
            "value": "Best trial: 0. Best value: 22700.8: 100%"
          }
        },
        "91fde5da00054e4db5aa731f773f87d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bbfbaab38e34abb903dfb30a9c1b746",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c5b091f5fec4999b420a66f98f191bd",
            "value": 20
          }
        },
        "7175c0738dca45d486e874602ef79f1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fad0de1fd1b745a984af5e75568013e4",
            "placeholder": "​",
            "style": "IPY_MODEL_8ab01a1022074ee1bba7c96cf5235a5e",
            "value": " 20/20 [03:46&lt;00:00, 14.14s/it]"
          }
        },
        "a7f7162843f34784b30f39bcbb15c818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fcfa4abc8a846718f1f339bcc7c0aef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1634abbdac7a4be4bef2a03ed6bbfd32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bbfbaab38e34abb903dfb30a9c1b746": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c5b091f5fec4999b420a66f98f191bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fad0de1fd1b745a984af5e75568013e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ab01a1022074ee1bba7c96cf5235a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}